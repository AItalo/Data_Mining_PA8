{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA 8: Association Rule Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we will implement the apriori algorithm for association rule mining to both discover association rules in a dataset and compute their \"interestingness\" statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Apriori Algorithm\n",
    "\n",
    "The first step is to implement the Apriori algorithm for use on further datasets. To do so, we will first borrow several dataset creation and cleaning functions from previous assignments, listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    f = open(filename, 'r')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def create_dataset(data):\n",
    "    data_r = data.splitlines()\n",
    "    dataset = []\n",
    "    for line in data_r:\n",
    "        instance = line.split(',')\n",
    "        dataset.append(instance)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then need several helper functions in order to follow the apriori algorithm. Their descriptions are as follows:\n",
    "\n",
    "* `compute_unique_values()`\n",
    "    * **Params**:\n",
    "        * `table` - The table to compute unique values for\n",
    "    * **Returns**:\n",
    "        * A list of all unique elements in the dataset, sorted\n",
    "        \n",
    "* `compute_subsets()`\n",
    "    * **Params**:\n",
    "        * `itemset` - The L_k-1 itemsets to compute L_k subsets for\n",
    "    * **Returns**:\n",
    "        * A list of all itemsets such that, for two elements of L_k-1 called A and B, the itemset is A + B[ -1 ] if all but the last element of A and B are identical\n",
    "        \n",
    "* `get_permutations()`\n",
    "    * **Params**:\n",
    "        * `itemset` - The set of elements to compute k-1 permutations for\n",
    "    * **Returns**:\n",
    "        * A list of all permutations of the data with length k-1\n",
    "        \n",
    "* `prepend_headers()`\n",
    "    * **Params**:\n",
    "        * `itemset` - A table of data to prepend headers to\n",
    "        * `headers` - The list of headers for each column to prepend to elements of `itemset`\n",
    "    * **Returns**:\n",
    "        * The data table where each element is prepended with the header for that column\n",
    "        \n",
    "* `prune_absent_subset()`\n",
    "    * **Params**:\n",
    "        * `Lk` - The list L_k-1 in the apriori algorithm\n",
    "        * `Ck` - The candidate set for L_k to be pruned\n",
    "    * **Returns**:\n",
    "        * A list of all elements of `Ck` for which all k-1 subsets exist within `Lk`\n",
    "        * In other words, `Ck` pruned so that all elements for which a subset of the element does not exist in L_k-1 are removed\n",
    "        \n",
    "* `calculate_support()`\n",
    "    * **Params**:\n",
    "        * `transactions` - The list of all transactions in the data\n",
    "        * `element` - The element to calculate support for\n",
    "    * **Returns**:\n",
    "        * The support for the given element in the data, described by the function $\\frac{count(S)}{N}$\n",
    "        \n",
    "* `prune_support()`\n",
    "    * **Params**:\n",
    "        * `transactions` - The list of all transactions in the data\n",
    "        * `Ck` - The candidate set to prune elements from\n",
    "        * `minsupport` - The minimum support value that an element can have before it is pruned\n",
    "    * **Returns**:\n",
    "        * `Ck` pruned such that all elements with a support of less than `minsupport` are removed\n",
    "        \n",
    "* `generate_rhs()`\n",
    "    * **Params**:\n",
    "        * `S` - The itemset to generate all possible RHS subsets for\n",
    "        * `k` - the length of generated RHS subsets; used for recursion\n",
    "    * **Returns**:\n",
    "        * A list of all possible RHS subsets for the given itemset\n",
    "        \n",
    "* `generate_rules()`\n",
    "    * **Params**:\n",
    "        * `L` - The superset of all possible itemsets generated from the apriori algorithm\n",
    "    * **Returns**:\n",
    "        * A list of rules in the form { \"lhs\" : [...], \"rhs\" : [...] }\n",
    "        \n",
    "* `prune_confidence()`\n",
    "    * **Params**:\n",
    "        * `transactions` - The data table of all transactions\n",
    "        * `rules` - A list of rules in the above format to be pruned\n",
    "        * `minconf` - The minimum confidence value a rule can have before it is pruned, calculated using the formula $\\frac{count(S)}{count(LHS)}$\n",
    "    * **Returns**:\n",
    "        * A list of all rules, pruned such that all rules with a confidence below `minconf` are pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unique_values(table):\n",
    "    I = set()\n",
    "    for row in table:\n",
    "        for elem in row:\n",
    "            I.add(elem)\n",
    "    return list(I)\n",
    "\n",
    "\n",
    "def compute_subsets(itemset):\n",
    "    L = []\n",
    "    for a in range(len(itemset)):\n",
    "        for b in range(a+1, len(itemset)):\n",
    "            A = itemset[a]\n",
    "            B = itemset[b]\n",
    "            if A != B and A[:-1] == B[:-1]:\n",
    "                AB = A + [B[-1]]\n",
    "                L.append(AB)\n",
    "    return L\n",
    "\n",
    "\n",
    "def get_permutations(itemset):\n",
    "    permutations = []\n",
    "    for i in range(len(itemset)):\n",
    "        permutation = itemset[:]\n",
    "        del permutation[i]\n",
    "        permutations.append(permutation)\n",
    "    return permutations\n",
    "\n",
    "\n",
    "def prepend_headers(itemset, headers):\n",
    "    prepended = []\n",
    "    for row in itemset:\n",
    "        prepended.append([headers[i] + \"=\" + row[i] for i in range(len(headers))])\n",
    "    return prepended\n",
    "\n",
    "\n",
    "def prune_absent_subset(Lk, Ck):\n",
    "    Ck_pruned = []\n",
    "    for elem in Ck:\n",
    "        prune = False\n",
    "        for permutation in get_permutations(elem):\n",
    "            if permutation not in Lk:\n",
    "                prune = True\n",
    "                break\n",
    "        if not prune:\n",
    "            Ck_pruned.append(elem)\n",
    "    return Ck_pruned\n",
    "\n",
    "\n",
    "def calculate_support(transactions, element):\n",
    "    count = 0\n",
    "    for item in transactions:\n",
    "        supported = True\n",
    "        for e in element:\n",
    "            if e not in item:\n",
    "                supported = False\n",
    "        if supported:\n",
    "            count += 1\n",
    "    return count / len(transactions)\n",
    "\n",
    "\n",
    "def prune_support(transactions, Ck, minsupport):\n",
    "    Ck_pruned = []\n",
    "    for element in Ck:\n",
    "        supp = calculate_support(transactions, element)\n",
    "        if supp >= minsupport:\n",
    "            Ck_pruned.append(element)\n",
    "    return Ck_pruned\n",
    "\n",
    "\n",
    "def generate_rhs(S, k):\n",
    "    if k == 1:\n",
    "        return [[x] for x in S]\n",
    "    else:\n",
    "        rhs = []\n",
    "        for i in range(len(S) + 1 - k):\n",
    "            rhs += [[S[i]] + x for x in generate_rhs(S[i+1:], k-1)]\n",
    "        return rhs\n",
    "\n",
    "\n",
    "def generate_rules(L):\n",
    "    rules = []\n",
    "    for subset in L:\n",
    "        for S in subset:\n",
    "            for i in range(1, len(S)):\n",
    "                rhs_i = generate_rhs(S, i)\n",
    "                for rhs in rhs_i:\n",
    "                    lhs = []\n",
    "                    for elem in S:\n",
    "                        if elem not in rhs:\n",
    "                            lhs.append(elem)\n",
    "                    \n",
    "                    rule = {\"lhs\":lhs, \"rhs\":rhs}\n",
    "                    rules.append(rule)\n",
    "    return rules\n",
    "\n",
    "\n",
    "def prune_confidence(transactions, rules, minconf):\n",
    "    pruned = []\n",
    "    for rule in rules:\n",
    "        count_s = 0\n",
    "        count_l = 0\n",
    "        lhs = rule[\"lhs\"]\n",
    "        rhs = rule[\"rhs\"]\n",
    "        for row in transactions:\n",
    "            both = True\n",
    "            left = True\n",
    "            for elem in lhs:\n",
    "                if elem not in row:\n",
    "                    left = False\n",
    "                    both = False\n",
    "            if left == True:\n",
    "                count_l += 1\n",
    "                for elem in rhs:\n",
    "                    if elem not in row:\n",
    "                        both = False\n",
    "                if both == True:\n",
    "                    count_s += 1\n",
    "        conf = count_s / count_l\n",
    "        if conf >= minconf:\n",
    "            pruned.append(rule)\n",
    "    return pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing the rules using apriori, we will also need to compute the statistics Support, Confidence, and Lift for each value. Here we will define a function to calculate these values:\n",
    "\n",
    "* `calculate_stats()`\n",
    "    * **Params**:\n",
    "        * `transactions` - The list of all transactions in the dataset\n",
    "        * `rules` - The list of all rules to calculate statistics for\n",
    "    * **Returns**:\n",
    "        * A 2D array of all of the rules and their corresponding statistics, such that each row takes the form `\\[rule, support, confidence, lift\\]\n",
    "        * Lift is calculated using the function $\\frac{support(S)}{support(LHS)\\times support(RHS)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(transactions, rules):\n",
    "    stats = []\n",
    "    for rule in rules:\n",
    "        count_l = 0\n",
    "        count_r = 0\n",
    "        count_s = 0\n",
    "        lhs = rule[\"lhs\"]\n",
    "        rhs = rule[\"rhs\"]\n",
    "        for row in transactions:\n",
    "            left = True\n",
    "            right = True\n",
    "            for elem in lhs:\n",
    "                if elem not in row:\n",
    "                    left = False\n",
    "            for elem in rhs:\n",
    "                if elem not in row:\n",
    "                    right = False\n",
    "            if left:\n",
    "                count_l += 1\n",
    "            if right:\n",
    "                count_r += 1\n",
    "            if left and right:\n",
    "                count_s += 1\n",
    "        support_l = count_l / len(transactions)\n",
    "        support_r = count_r / len(transactions)\n",
    "        support = count_s / len(transactions)\n",
    "        confidence = count_s / count_l\n",
    "        lift = support / (support_l * support_r)\n",
    "        stats.append([rule, support, confidence, lift])\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define our apriori algorithm, as well as a function to pretty print the returned rules and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def apriori(transactions, minsupport, minconf, headers=[]):\n",
    "    if headers != []:\n",
    "        transactions = prepend_headers(transactions, headers)\n",
    "    supported = []\n",
    "    I = sorted(compute_unique_values(transactions))\n",
    "    L = [[x] for x in I]\n",
    "    L = prune_support(transactions, L, minsupport)\n",
    "    while L != []:\n",
    "        Ck = compute_subsets(L)\n",
    "        Ck = prune_absent_subset(L, Ck)\n",
    "        L = prune_support(transactions, Ck, minsupport)\n",
    "        if L != []:\n",
    "            supported.append(L)\n",
    "    rules = generate_rules(supported)\n",
    "    rules = prune_confidence(transactions, rules, minconf)\n",
    "    stats = calculate_stats(transactions, rules)\n",
    "    return stats\n",
    "\n",
    "def pprint(stats):\n",
    "    pretty_stats = []\n",
    "    for rule in stats:\n",
    "        string = \"\"\n",
    "        lhs = rule[0][\"lhs\"]\n",
    "        rhs = rule[0][\"rhs\"]\n",
    "        string += lhs[0]\n",
    "        if len(lhs) > 1:\n",
    "            for i in range(1, len(lhs)):\n",
    "                string += \" AND \" + lhs[i]\n",
    "        string += \" => \" + rhs[0]\n",
    "        if len(rhs) > 1:\n",
    "            for i in range(1, len(rhs)):\n",
    "                string += \" AND \" + rhs[i]\n",
    "        pretty_stats.append([string]+rule[1:])\n",
    "    header = [\"association rule\", \"support\", \"confidence\", \"lift\"]\n",
    "    print(tabulate(pretty_stats, headers=header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test this function, we will run it on two datasets used in class. The first is the interview dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"level\", \"lang\", \"tweets\", \"phd\", \"interviewed_well\"]\n",
    "table = [\n",
    "        [\"Senior\", \"Java\", \"no\", \"no\", \"False\"],\n",
    "        [\"Senior\", \"Java\", \"no\", \"yes\", \"False\"],\n",
    "        [\"Mid\", \"Python\", \"no\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"Python\", \"no\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"R\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"R\", \"yes\", \"yes\", \"False\"],\n",
    "        [\"Mid\", \"R\", \"yes\", \"yes\", \"True\"],\n",
    "        [\"Senior\", \"Python\", \"no\", \"no\", \"False\"],\n",
    "        [\"Senior\", \"R\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"Python\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Senior\", \"Python\", \"yes\", \"yes\", \"True\"],\n",
    "        [\"Mid\", \"Python\", \"no\", \"yes\", \"True\"],\n",
    "        [\"Mid\", \"Java\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"Python\", \"no\", \"yes\", \"False\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a minsupport of 0.25 and a minconf of 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "association rule                                  support    confidence     lift\n",
      "----------------------------------------------  ---------  ------------  -------\n",
      "interviewed_well=False => tweets=no              0.285714      0.8       1.6\n",
      "level=Mid => interviewed_well=True               0.285714      1         1.55556\n",
      "phd=no => interviewed_well=True                  0.428571      0.75      1.16667\n",
      "tweets=yes => interviewed_well=True              0.428571      0.857143  1.33333\n",
      "lang=R => tweets=yes                             0.285714      1         2\n",
      "phd=no AND tweets=yes => interviewed_well=True   0.285714      1         1.55556\n"
     ]
    }
   ],
   "source": [
    "pprint(apriori(table, 0.25, 0.75, headers=header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will run our algorithm on the market basket analysis transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [\n",
    "    [\"b\", \"c\", \"m\"],\n",
    "    [\"b\", \"c\", \"e\", \"m\", \"s\"],\n",
    "    [\"b\"],\n",
    "    [\"c\", \"e\", \"s\"],\n",
    "    [\"c\"],\n",
    "    [\"b\", \"c\", \"s\"],\n",
    "    [\"c\", \"e\", \"s\"],\n",
    "    [\"c\", \"e\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same values of 0.25 and 0.75 for minsupport and minconf, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "association rule      support    confidence      lift\n",
      "------------------  ---------  ------------  --------\n",
      "b => c                  0.375          0.75  0.857143\n",
      "m => b                  0.25           1     2\n",
      "e => c                  0.5            1     1.14286\n",
      "m => c                  0.25           1     1.14286\n",
      "s => c                  0.5            1     1.14286\n",
      "s => e                  0.375          0.75  1.5\n",
      "e => s                  0.375          0.75  1.5\n",
      "c AND m => b            0.25           1     2\n",
      "b AND m => c            0.25           1     1.14286\n",
      "m => b AND c            0.25           1     2.66667\n",
      "b AND s => c            0.25           1     1.14286\n",
      "e AND s => c            0.375          1     1.14286\n",
      "c AND s => e            0.375          0.75  1.5\n",
      "c AND e => s            0.375          0.75  1.5\n",
      "s => c AND e            0.375          0.75  1.5\n",
      "e => c AND s            0.375          0.75  1.5\n"
     ]
    }
   ],
   "source": [
    "pprint(apriori(transactions, 0.25, 0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Titanic Dataset\n",
    "\n",
    "For this step, we will run our apriori algorithm over the titanic dataset. First, we will need to retrieve and clean the data for use in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data = create_dataset(read_data(\"titanic_data.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can simply plug our dataset, along with a list of headers, into our apriori algorithm and have it pretty print the rules and associated statistics. Since this is a much larger dataset, we will restrict it to the most interesting rules, and will therefore set our minsupport as 0.3 and our minconf as 0.9, ensuring that each rule we produce is well supported in the dataset and we are reasonably confident it is accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "association rule                                            support    confidence     lift\n",
      "--------------------------------------------------------  ---------  ------------  -------\n",
      "class=third => age=adult                                   0.40209       1         1.0521\n",
      "sex=female => age=adult                                    0.757383      0.963027  1.0132\n",
      "survived=yes => age=adult                                  0.653339      0.965101  1.01539\n",
      "class=third => sex=female                                  0.39164       0.974011  1.23847\n",
      "survived=yes => sex=female                                 0.619718      0.915436  1.16399\n",
      "class=third AND sex=female => age=adult                    0.39164       1         1.0521\n",
      "age=adult AND class=third => sex=female                    0.39164       0.974011  1.23847\n",
      "class=third => age=adult AND sex=female                    0.39164       0.974011  1.28602\n",
      "class=third AND survived=yes => age=adult                  0.30577       1         1.0521\n",
      "sex=female AND survived=yes => age=adult                   0.603816      0.97434   1.02511\n",
      "age=adult AND survived=yes => sex=female                   0.603816      0.9242    1.17514\n",
      "class=third AND survived=yes => sex=female                 0.304407      0.995542  1.26585\n",
      "class=third AND sex=female AND survived=yes => age=adult   0.304407      1         1.0521\n",
      "age=adult AND class=third AND survived=yes => sex=female   0.304407      0.995542  1.26585\n",
      "class=third AND survived=yes => age=adult AND sex=female   0.304407      0.995542  1.31445\n"
     ]
    }
   ],
   "source": [
    "titanic_headers = [\"class\", \"age\", \"sex\", \"survived\"]\n",
    "pprint(apriori(titanic_data, 0.3, 0.9, headers=titanic_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these rules, we can see some interesting associations. Some make sense, such as being an adult and surviving implies you are a female with very good support and confidence. Some are more odd though, such as the fact that if you are a third class female passenger, our rules imply you are an adult with a confidence of 1, meaning there were no instances of third class female children at all, which is interesting in tandem with the fact that being third class is heavily correlated with being female, meaning there were very few third class male passengers to begin with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Mushroom Dataset\n",
    "\n",
    "For this step, we will once again run our apriori algorithm, this time over the agaricus-lepiota dataset. This dataset contains information about different types of mushrooms and whether they are edible or posionous. The breakdown of all attributes for the dataset is as follows, copied from the PA description:\n",
    "\n",
    "1. The class label edible=e, poisonous=p\n",
    "2. cap-shape bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s\n",
    "3. cap-surface fibrous=f, grooves=g, scaly=y, smooth=s\n",
    "4. cap-color brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y\n",
    "5. bruises? bruises=t, no=f\n",
    "6. odor almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s\n",
    "7. gill-attachment attached=a, descending=d, free=f, notched=n\n",
    "8. gill-spacing close=c, crowded=w, distant=d\n",
    "9. gill-size broad=b, narrow=n\n",
    "10. gill-color black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y\n",
    "11. stalk-shape enlarging=e, tapering=t\n",
    "12. stalk-root bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r\n",
    "13. stalk-surface-above-ring fibrous=f,scaly=y,silky=k,smooth=s\n",
    "14. stalk-surface-below-ring fibrous=f,scaly=y,silky=k,smooth=s\n",
    "15. stalk-color-above-ring brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y\n",
    "16. stalk-color-below-ring brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y\n",
    "17. veil-type partial=p, universal=u\n",
    "18. veil-color brown=n, orange=o, white=w, yellow=y\n",
    "19. ring-number none=n, one=o, two=t\n",
    "20. ring-type cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z\n",
    "21. spore-print-color black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y\n",
    "22. population abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y\n",
    "23. habitat grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d\n",
    "\n",
    "As before, our first step is to grab the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mushroom_data = create_dataset(read_data(\"agaricus-lepiota.txt\"))\n",
    "\n",
    "mushroom_headers = [\"class\", \"cap-shape\", \"cap-surface\", \"cap-color\", \"bruises?\", \"odor\", \"gill-attachment\", \\\n",
    "          \"gill-spacing\", \"gill-size\", \"gill-color\", \"stalk-shape\", \"stalk-root\", \"stalk-surface-above-ring\", \\\n",
    "          \"stalk-surface-below-ring\", \"stalk-color-above-ring\", \"stalk-color-below-ring\", \"veil-type\", \\\n",
    "          \"veil-color\", \"ring-number\", \"ring-type\", \"spore-print-color\", \"population\", \"habitat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the size of this data, we will need to use feature selection to prune the data to only a subset of the given attributes. For this purpose, we will write the helper function `feature_selection()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(dataset, features, headers):\n",
    "    pruned = []\n",
    "    pruned_headers = []\n",
    "    for row in dataset:\n",
    "        pruned_row = []\n",
    "        for i in features:\n",
    "            pruned_row.append(row[i])\n",
    "        pruned.append(pruned_row)\n",
    "    for i in features:\n",
    "        pruned_headers.append(headers[i])\n",
    "    return pruned, pruned_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this step, we will choose the following attributes to keep:\n",
    "\n",
    "* edible\n",
    "* cap-shape\n",
    "* cap-surface\n",
    "* cap-color\n",
    "* odor\n",
    "* gill-attachment\n",
    "* gill-size\n",
    "* gill-color\n",
    "* ring-number\n",
    "* ring-type\n",
    "* spore-print\n",
    "* population\n",
    "* habitat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mushroom_data_pruned, mushroom_headers_pruned = feature_selection(mushroom_data, [0, 1, 2, 3, 5, 6, 8, 9, 18, 19, 20, 21, 22], mushroom_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the apriori algorithm over the pruned dataset. Since this is a huge dataset with many attributes, we will want to restrict our rule generation significantly. Therefore, we will only accept rules with a support of at least 0.6, and a confidence of 0.95, to avoid generating a huge number of rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "association rule                                      support    confidence      lift\n",
      "--------------------------------------------------  ---------  ------------  --------\n",
      "class=e => gill-attachment=f                         0.618001      1         1.0032\n",
      "class=e => ring-number=o                             0.609497      0.986239  1.01427\n",
      "gill-size=b => gill-attachment=f                     0.872077      0.996356  0.999544\n",
      "ring-number=o => gill-attachment=f                   0.97236       1         1.0032\n",
      "gill-attachment=f => ring-number=o                   0.97236       0.975471  1.0032\n",
      "ring-type=p => gill-attachment=f                     0.618001      1         1.0032\n",
      "gill-size=b => ring-number=o                         0.847626      0.968421  0.995949\n",
      "class=e AND ring-number=o => gill-attachment=f       0.609497      1         1.0032\n",
      "class=e AND gill-attachment=f => ring-number=o       0.609497      0.986239  1.01427\n",
      "class=e => gill-attachment=f AND ring-number=o       0.609497      0.986239  1.01427\n",
      "gill-size=b AND ring-number=o => gill-attachment=f   0.847626      1         1.0032\n",
      "gill-attachment=f AND gill-size=b => ring-number=o   0.847626      0.971963  0.999591\n",
      "gill-size=b => gill-attachment=f AND ring-number=o   0.847626      0.968421  0.995949\n"
     ]
    }
   ],
   "source": [
    "pprint(apriori(mushroom_data_pruned, 0.6, 0.95, headers=mushroom_headers_pruned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
