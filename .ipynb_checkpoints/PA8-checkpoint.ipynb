{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA 8: Association Rule Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we will implement the apriori algorithm for association rule mining to both discover association rules in a dataset and compute their \"interestingness\" statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Apriori Algorithm\n",
    "\n",
    "The first step is to implement the Apriori algorithm for use on further datasets. To do so, we will first borrow several dataset creation and cleaning functions from previous assignments, listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    f = open(filename, 'r')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def create_dataset(data):\n",
    "    data_r = data.splitlines()\n",
    "    dataset = []\n",
    "    for line in data_r:\n",
    "        instance = line.split(',')\n",
    "        dataset.append(instance)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then need several helper functions in order to follow the apriori algorithm. Their descriptions are as follows:\n",
    "\n",
    "* `compute_unique_values()`\n",
    "    * **Params**:\n",
    "        * `table` - The table to compute unique values for\n",
    "    * **Returns**:\n",
    "        * A list of all unique elements in the dataset, sorted\n",
    "        \n",
    "* `compute_subsets()`\n",
    "    * **Params**:\n",
    "        * `itemset` - The L_k-1 itemsets to compute L_k subsets for\n",
    "    * **Returns**:\n",
    "        * A list of all itemsets such that, for two elements of L_k-1 called A and B, the itemset is A + B[ -1 ] if all but the last element of A and B are identical\n",
    "        \n",
    "* `get_permutations()`\n",
    "    * **Params**:\n",
    "        * `itemset` - The set of elements to compute k-1 permutations for\n",
    "    * **Returns**:\n",
    "        * A list of all permutations of the data with length k-1\n",
    "        \n",
    "* `prepend_headers()`\n",
    "    * **Params**:\n",
    "        * `itemset` - A table of data to prepend headers to\n",
    "        * `headers` - The list of headers for each column to prepend to elements of `itemset`\n",
    "    * **Returns**:\n",
    "        * The data table where each element is prepended with the header for that column\n",
    "        \n",
    "* `prune_absent_subset()`\n",
    "    * **Params**:\n",
    "        * `Lk` - The list L_k-1 in the apriori algorithm\n",
    "        * `Ck` - The candidate set for L_k to be pruned\n",
    "    * **Returns**:\n",
    "        * A list of all elements of `Ck` for which all k-1 subsets exist within `Lk`\n",
    "        * In other words, `Ck` pruned so that all elements for which a subset of the element does not exist in L_k-1 are removed\n",
    "        \n",
    "* `calculate_support()`\n",
    "    * **Params**:\n",
    "        * `transactions` - The list of all transactions in the data\n",
    "        * `element` - The element to calculate support for\n",
    "    * **Returns**:\n",
    "        * The support for the given element in the data, described by the function $\\frac{count(S)}{N}$\n",
    "        \n",
    "* `prune_support()`\n",
    "    * **Params**:\n",
    "        * `transactions` - The list of all transactions in the data\n",
    "        * `Ck` - The candidate set to prune elements from\n",
    "        * `minsupport` - The minimum support value that an element can have before it is pruned\n",
    "    * **Returns**:\n",
    "        * `Ck` pruned such that all elements with a support of less than `minsupport` are removed\n",
    "        \n",
    "* `generate_rhs()`\n",
    "    * **Params**:\n",
    "        * `S` - The itemset to generate all possible RHS subsets for\n",
    "        * `k` - the length of generated RHS subsets; used for recursion\n",
    "    * **Returns**:\n",
    "        * A list of all possible RHS subsets for the given itemset\n",
    "        \n",
    "* `generate_rules()`\n",
    "    * **Params**:\n",
    "        * `L` - The superset of all possible itemsets generated from the apriori algorithm\n",
    "    * **Returns**:\n",
    "        * A list of rules in the form { \"lhs\" : [...], \"rhs\" : [...] }\n",
    "        \n",
    "* `prune_confidence()`\n",
    "    * **Params**:\n",
    "        * `transactions` - The data table of all transactions\n",
    "        * `rules` - A list of rules in the above format to be pruned\n",
    "        * `minconf` - The minimum confidence value a rule can have before it is pruned, calculated using the formula $\\frac{count(S)}{count(LHS)}$\n",
    "    * **Returns**:\n",
    "        * A list of all rules, pruned such that all rules with a confidence below `minconf` are pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unique_values(table):\n",
    "    I = set()\n",
    "    for row in table:\n",
    "        for elem in row:\n",
    "            I.add(elem)\n",
    "    return list(I)\n",
    "\n",
    "\n",
    "def compute_subsets(itemset):\n",
    "    L = []\n",
    "    for a in range(len(itemset)):\n",
    "        for b in range(a+1, len(itemset)):\n",
    "            A = itemset[a]\n",
    "            B = itemset[b]\n",
    "            if A != B and A[:-1] == B[:-1]:\n",
    "                AB = A + [B[-1]]\n",
    "                L.append(AB)\n",
    "    return L\n",
    "\n",
    "\n",
    "def get_permutations(itemset):\n",
    "    permutations = []\n",
    "    for i in range(len(itemset)):\n",
    "        permutation = itemset[:]\n",
    "        del permutation[i]\n",
    "        permutations.append(permutation)\n",
    "    return permutations\n",
    "\n",
    "\n",
    "def prepend_headers(itemset, headers):\n",
    "    prepended = []\n",
    "    for row in itemset:\n",
    "        prepended.append([headers[i] + \"=\" + row[i] for i in range(len(headers))])\n",
    "    return prepended\n",
    "\n",
    "\n",
    "def prune_absent_subset(Lk, Ck):\n",
    "    Ck_pruned = []\n",
    "    for elem in Ck:\n",
    "        prune = False\n",
    "        for permutation in get_permutations(elem):\n",
    "            if permutation not in Lk:\n",
    "                prune = True\n",
    "                break\n",
    "        if not prune:\n",
    "            Ck_pruned.append(elem)\n",
    "    return Ck_pruned\n",
    "\n",
    "\n",
    "def calculate_support(transactions, element):\n",
    "    count = 0\n",
    "    for item in transactions:\n",
    "        supported = True\n",
    "        for e in element:\n",
    "            if e not in item:\n",
    "                supported = False\n",
    "        if supported:\n",
    "            count += 1\n",
    "    return count / len(transactions)\n",
    "\n",
    "\n",
    "def prune_support(transactions, Ck, minsupport):\n",
    "    Ck_pruned = []\n",
    "    for element in Ck:\n",
    "        supp = calculate_support(transactions, element)\n",
    "        if supp >= minsupport:\n",
    "            Ck_pruned.append(element)\n",
    "    return Ck_pruned\n",
    "\n",
    "\n",
    "def generate_rhs(S, k):\n",
    "    if k == 1:\n",
    "        return [[x] for x in S]\n",
    "    else:\n",
    "        rhs = []\n",
    "        for i in range(len(S) + 1 - k):\n",
    "            rhs += [[S[i]] + x for x in generate_rhs(S[i+1:], k-1)]\n",
    "        return rhs\n",
    "\n",
    "\n",
    "def generate_rules(L):\n",
    "    rules = []\n",
    "    for subset in L:\n",
    "        for S in subset:\n",
    "            for i in range(1, len(S)):\n",
    "                rhs_i = generate_rhs(S, i)\n",
    "                for rhs in rhs_i:\n",
    "                    lhs = []\n",
    "                    for elem in S:\n",
    "                        if elem not in rhs:\n",
    "                            lhs.append(elem)\n",
    "                    \n",
    "                    rule = {\"lhs\":lhs, \"rhs\":rhs}\n",
    "                    rules.append(rule)\n",
    "    return rules\n",
    "\n",
    "\n",
    "def prune_confidence(transactions, rules, minconf):\n",
    "    pruned = []\n",
    "    for rule in rules:\n",
    "        count_s = 0\n",
    "        count_l = 0\n",
    "        lhs = rule[\"lhs\"]\n",
    "        rhs = rule[\"rhs\"]\n",
    "        for row in transactions:\n",
    "            both = True\n",
    "            left = True\n",
    "            for elem in lhs:\n",
    "                if elem not in row:\n",
    "                    left = False\n",
    "                    both = False\n",
    "            if left == True:\n",
    "                count_l += 1\n",
    "                for elem in rhs:\n",
    "                    if elem not in row:\n",
    "                        both = False\n",
    "                if both == True:\n",
    "                    count_s += 1\n",
    "        conf = count_s / count_l\n",
    "        if conf >= minconf:\n",
    "            pruned.append(rule)\n",
    "    return pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing the rules using apriori, we will also need to compute the statistics Support, Confidence, and Lift for each value. Here we will define a function to calculate these values:\n",
    "\n",
    "* `calculate_stats()`\n",
    "    * **Params**:\n",
    "        * `transactions` - The list of all transactions in the dataset\n",
    "        * `rules` - The list of all rules to calculate statistics for\n",
    "    * **Returns**:\n",
    "        * A 2D array of all of the rules and their corresponding statistics, such that each row takes the form `\\[rule, support, confidence, lift\\]\n",
    "        * Lift is calculated using the function $\\frac{support(S)}{support(LHS)\\times support(RHS)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(transactions, rules):\n",
    "    stats = []\n",
    "    for rule in rules:\n",
    "        count_l = 0\n",
    "        count_r = 0\n",
    "        count_s = 0\n",
    "        lhs = rule[\"lhs\"]\n",
    "        rhs = rule[\"rhs\"]\n",
    "        for row in transactions:\n",
    "            left = True\n",
    "            right = True\n",
    "            for elem in lhs:\n",
    "                if elem not in row:\n",
    "                    left = False\n",
    "            for elem in rhs:\n",
    "                if elem not in row:\n",
    "                    right = False\n",
    "            if left:\n",
    "                count_l += 1\n",
    "            if right:\n",
    "                count_r += 1\n",
    "            if left and right:\n",
    "                count_s += 1\n",
    "        support_l = count_l / len(transactions)\n",
    "        support_r = count_r / len(transactions)\n",
    "        support = count_s / len(transactions)\n",
    "        confidence = count_s / count_l\n",
    "        lift = support / (support_l * support_r)\n",
    "        stats.append([rule, support, confidence, lift])\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define our apriori algorithm, as well as a function to pretty print the returned rules and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def apriori(transactions, minsupport, minconf, headers=[]):\n",
    "    if headers != []:\n",
    "        transactions = prepend_headers(transactions, headers)\n",
    "    supported = []\n",
    "    I = sorted(compute_unique_values(transactions))\n",
    "    L = [[x] for x in I]\n",
    "    L = prune_support(transactions, L, minsupport)\n",
    "    while L != []:\n",
    "        Ck = compute_subsets(L)\n",
    "        Ck = prune_absent_subset(L, Ck)\n",
    "        L = prune_support(transactions, Ck, minsupport)\n",
    "        if L != []:\n",
    "            supported.append(L)\n",
    "    rules = generate_rules(supported)\n",
    "    rules = prune_confidence(transactions, rules, minconf)\n",
    "    stats = calculate_stats(transactions, rules)\n",
    "    return stats\n",
    "\n",
    "def pprint(stats):\n",
    "    pretty_stats = []\n",
    "    for rule in stats:\n",
    "        string = \"\"\n",
    "        lhs = rule[0][\"lhs\"]\n",
    "        rhs = rule[0][\"rhs\"]\n",
    "        string += lhs[0]\n",
    "        if len(lhs) > 1:\n",
    "            for i in range(1, len(lhs)):\n",
    "                string += \" AND \" + lhs[i]\n",
    "        string += \" => \" + rhs[0]\n",
    "        if len(rhs) > 1:\n",
    "            string += \" AND \" + rhs[i]\n",
    "        pretty_stats.append([string]+rule[1:])\n",
    "    header = [\"association rule\", \"support\", \"confidence\", \"lift\"]\n",
    "    print(tabulate(pretty_stats, headers=header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test this function, we will run it on two datasets used in class. The first is the interview dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"level\", \"lang\", \"tweets\", \"phd\", \"interviewed_well\"]\n",
    "table = [\n",
    "        [\"Senior\", \"Java\", \"no\", \"no\", \"False\"],\n",
    "        [\"Senior\", \"Java\", \"no\", \"yes\", \"False\"],\n",
    "        [\"Mid\", \"Python\", \"no\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"Python\", \"no\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"R\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"R\", \"yes\", \"yes\", \"False\"],\n",
    "        [\"Mid\", \"R\", \"yes\", \"yes\", \"True\"],\n",
    "        [\"Senior\", \"Python\", \"no\", \"no\", \"False\"],\n",
    "        [\"Senior\", \"R\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"Python\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Senior\", \"Python\", \"yes\", \"yes\", \"True\"],\n",
    "        [\"Mid\", \"Python\", \"no\", \"yes\", \"True\"],\n",
    "        [\"Mid\", \"Java\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"Python\", \"no\", \"yes\", \"False\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a minsupport of 0.25 and a minconf of 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "association rule                                  support    confidence     lift\n",
      "----------------------------------------------  ---------  ------------  -------\n",
      "interviewed_well=False => tweets=no              0.285714      0.8       1.6\n",
      "level=Mid => interviewed_well=True               0.285714      1         1.55556\n",
      "phd=no => interviewed_well=True                  0.428571      0.75      1.16667\n",
      "tweets=yes => interviewed_well=True              0.428571      0.857143  1.33333\n",
      "lang=R => tweets=yes                             0.285714      1         2\n",
      "phd=no AND tweets=yes => interviewed_well=True   0.285714      1         1.55556\n"
     ]
    }
   ],
   "source": [
    "pprint(apriori(table, 0.25, 0.75, headers=header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will run our algorithm on the market basket analysis transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [\n",
    "    [\"b\", \"c\", \"m\"],\n",
    "    [\"b\", \"c\", \"e\", \"m\", \"s\"],\n",
    "    [\"b\"],\n",
    "    [\"c\", \"e\", \"s\"],\n",
    "    [\"c\"],\n",
    "    [\"b\", \"c\", \"s\"],\n",
    "    [\"c\", \"e\", \"s\"],\n",
    "    [\"c\", \"e\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same values of 0.25 and 0.75 for minsupport and minconf, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "association rule      support    confidence      lift\n",
      "------------------  ---------  ------------  --------\n",
      "b => c                  0.375          0.75  0.857143\n",
      "m => b                  0.25           1     2\n",
      "e => c                  0.5            1     1.14286\n",
      "m => c                  0.25           1     1.14286\n",
      "s => c                  0.5            1     1.14286\n",
      "s => e                  0.375          0.75  1.5\n",
      "e => s                  0.375          0.75  1.5\n",
      "c AND m => b            0.25           1     2\n",
      "b AND m => c            0.25           1     1.14286\n",
      "m => b AND c            0.25           1     2.66667\n",
      "b AND s => c            0.25           1     1.14286\n",
      "e AND s => c            0.375          1     1.14286\n",
      "c AND s => e            0.375          0.75  1.5\n",
      "c AND e => s            0.375          0.75  1.5\n",
      "s => c AND e            0.375          0.75  1.5\n",
      "e => c AND s            0.375          0.75  1.5\n"
     ]
    }
   ],
   "source": [
    "pprint(apriori(transactions, 0.25, 0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Titanic Dataset\n",
    "\n",
    "For this step, we will run our apriori algorithm over the titanic dataset. First, we will need to retrieve and clean the data for use in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numerify_instance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d697795cfeb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtitanic_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"titanic_data.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-ecdde85838d0>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset_r\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mnewInstance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerify_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewInstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'numerify_instance' is not defined"
     ]
    }
   ],
   "source": [
    "titanic_data = create_dataset(read_data(\"titanic_data.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
